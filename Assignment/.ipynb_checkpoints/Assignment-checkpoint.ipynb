{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def opinion_extractor(aspect_token, parsed_sentence):\n",
    "    \n",
    "    # Your function will have 3 steps:\n",
    "    \n",
    "    # i. Initialise a list of opinions\n",
    "    opinions = []\n",
    "    isNeg = False\n",
    "    \n",
    "    # ii. Find opinions\n",
    "    #opinions += [dependant.form for dependant in parsed_sentence.get_dependants(aspect_token) if dependant.deprel == \"amod\" and dependant.pos == \"JJ\"] \n",
    "    for dependant in parsed_sentence.get_dependants(aspect_token):\n",
    "        if (dependant.deprel == \"neg\"):\n",
    "            isNeg = True\n",
    "        if (dependant.deprel == \"amod\" and dependant.pos == \"JJ\"):\n",
    "            opinions += [thing(dependant, isNeg)]\n",
    "    \n",
    "    if(aspect_token.deprel == \"nsubj\"):\n",
    "        head = parsed_sentence.get_head(aspect_token)\n",
    "        if(head.pos == \"JJ\"):\n",
    "            opinions += [thing(head, isNeg)]\n",
    "            opinions += getMore(head, isNeg)\n",
    "\n",
    "                    \n",
    "    \n",
    "    # You can continue to add to \"opinions\". Remember you can get the head of a token, and filter by PoS tag or Deprel too!\n",
    "    #opinions = [token for token in opinions if token not ignoredWord(token)]\n",
    "    \n",
    "    # iii. Return the (possibly empty) list of opinions\n",
    "    return opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ignoredWord(check):\n",
    "    words = [\"main\", \"special\", \"peripheral\", \"other\", \"same\"]\n",
    "    ignore = False\n",
    "    for word in words:\n",
    "        if(check == word):\n",
    "            ignore = True\n",
    "    return ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def thing(token, isNeg): #token is an adjective\n",
    "    opinion = token.form\n",
    "    for dependant in parsed_sentence.get_dependants(token):\n",
    "        if (dependant.deprel == \"advmod\"):\n",
    "            opinion = dependant.form + \"-\" + token.form #adverb + \"-\" + adjective\n",
    "        if (dependant.deprel == \"neg\"):\n",
    "            isNeg = True\n",
    "    if (isNeg == True):\n",
    "        opinion = \"not-\" + opinion\n",
    "    return opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMore(token, isNeg):\n",
    "    cList = []\n",
    "    for dependant in parsed_sentence.get_dependants(token):\n",
    "        if (dependant.deprel == \"conj\" or dependant.deprel == \"ccomp\"):\n",
    "            cList += [thing(dependant, isNeg)]\n",
    "    return cList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706 sentences out of 3119 contained features\n"
     ]
    }
   ],
   "source": [
    "from sussex_nltk.parse import load_parsed_dvd_sentences, load_parsed_example_sentences\n",
    "\n",
    "aspect = \"plot\"   # Set this to the aspect token you're interested in\n",
    "save_file_path = \"output.txt\"    # Set this to the location of the file you wish to create/overwrite with the saved output.\n",
    "\n",
    "# Tracking these numbers will allow us to see what proportion of sentences we discovered features in\n",
    "sentences_with_discovered_features = 0  # Number of sentences we discovered features in\n",
    "total_sentences = 0  # Total number of sentences \n",
    "\n",
    "# This is a \"with statement\", it invokes a context manager, which handles the opening and closing of resources (like files)\n",
    "with open(save_file_path, \"w\") as save_file:  # The 'w' says that we want to write to the file\n",
    "    \n",
    "    # Iterate over all the parsed sentences\n",
    "    for parsed_sentence in load_parsed_dvd_sentences(aspect): #example...() or dvd...(aspect)   !!!!!\n",
    "        \n",
    "        total_sentences += 1  # We've seen another sentence\n",
    "        \n",
    "        opinions = [] # Make a list for holding any opinions we extract in this sentence\n",
    "\n",
    "        # Iterate over each of the aspect tokens in the sentences (in case there is more than one)\n",
    "        for aspect_token in parsed_sentence.get_query_tokens(aspect):\n",
    "            \n",
    "            # Call your opinion extractor\n",
    "            opinions += opinion_extractor(aspect_token, parsed_sentence)\n",
    "        \n",
    "        # If we found any opinions, write to the output file what we know.\n",
    "        if opinions: \n",
    "            # Currently, the sentence will only be printed if opinions were found. But if you want to know what you're missing,\n",
    "            # you could move the sentence printing outside the if-statement\n",
    "            \n",
    "            # Print a separator and the raw unparsed sentence\n",
    "            save_file.write(\"--- Sentence: %s ---\\n\" % parsed_sentence.raw())  # \"\\n\" starts a new line\n",
    "            # Print the parsed sentence\n",
    "            save_file.write(\"%s\\n\" % parsed_sentence) \n",
    "            # Print opinions extracted\n",
    "            save_file.write(\"Opinions: %s\\n\" % opinions)\n",
    "            \n",
    "            sentences_with_discovered_features += 1  # We've found features in another sentence\n",
    "            \n",
    "print \"%s sentences out of %s contained features\" % (sentences_with_discovered_features, total_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 sentences printed\n"
     ]
    }
   ],
   "source": [
    "#!!!!!!!!!!!!!!---------PRINT ALL TESTER---------!!!!!!!!!!!!!!!!!\n",
    "from sussex_nltk.parse import load_parsed_dvd_sentences, load_parsed_example_sentences\n",
    "\n",
    "aspect = \"plot\"   # Set this to the aspect token you're interested in\n",
    "save_file_path = \"output.txt\"    # Set this to the location of the file you wish to create/overwrite with the saved output.\n",
    "\n",
    "# Tracking these numbers will allow us to see what proportion of sentences we discovered features in\n",
    "sentences_with_discovered_features = 0  # Number of sentences we discovered features in\n",
    "total_sentences = 0  # Total number of sentences \n",
    "\n",
    "# This is a \"with statement\", it invokes a context manager, which handles the opening and closing of resources (like files)\n",
    "with open(save_file_path, \"w\") as save_file:  # The 'w' says that we want to write to the file\n",
    "    \n",
    "    # Iterate over all the parsed sentences\n",
    "    for parsed_sentence in load_parsed_example_sentences(): #example...() or dvd...(aspect)   !!!!!\n",
    "        \n",
    "        total_sentences += 1  # We've seen another sentence\n",
    "        \n",
    "        opinions = [] # Make a list for holding any opinions we extract in this sentence\n",
    "\n",
    "        # Iterate over each of the aspect tokens in the sentences (in case there is more than one)\n",
    "        for aspect_token in parsed_sentence.get_query_tokens(aspect):\n",
    "            \n",
    "            # Call your opinion extractor\n",
    "            opinions += opinion_extractor(aspect_token, parsed_sentence)\n",
    "        \n",
    "        # If we found any opinions, write to the output file what we know. \n",
    "        # Currently, the sentence will only be printed if opinions were found. But if you want to know what you're missing,\n",
    "        # you could move the sentence printing outside the if-statement\n",
    "            \n",
    "        # Print a separator and the raw unparsed sentence\n",
    "        save_file.write(\"--- Sentence: %s ---\\n\" % parsed_sentence.raw())  # \"\\n\" starts a new line\n",
    "        # Print the parsed sentence\n",
    "        save_file.write(\"%s\\n\" % parsed_sentence) \n",
    "        # Print opinions extracted\n",
    "        save_file.write(\"Opinions: %s\\n\" % opinions)\n",
    "            \n",
    "        sentences_with_discovered_features += 1  # We've found features in another sentence\n",
    "            \n",
    "print \"%s sentences printed\" % (total_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
